# Geoint Sprint

# Introducing eo-learn [Tuto 0](https://medium.com/sentinel-hub/introducing-eo-learn-ab37f2869f5c)

LULC : Land Use and Land Cover (huma, activities and natural elements).

EO data : Earth Observation Data

eo-learn : main resource to analyse opensource 

ROI : Region of Interest

<img src="https://miro.medium.com/max/1144/1*KZ1yB5I7p_bDgM-quvF5oQ.png" alt="Image for post" style="zoom:70%;" />

Main blocks of `eo-learn` :

- `EOPatch` : dict of **data** through numpy arrays and **shapes** through Shapely. Defined by : **coordinates** of bounding box and **time interval**.
  - All for time dependent spatial information : *Sentinel-2, Landsat 8, Sentinel-1 bands, cloud mask*. 
  - Or time independent  spatial information : *Digital Elevation Model, target LULC maps, count of valid pixels..*
  - Or time dependent AND time independent scalar information : *labels change detection, sun angles...*

> note : raster (trame) is pixels like in png, jpeg and vector is paths.

<img src="https://miro.medium.com/max/1030/1*pTOOYBGAmCrfLy5zOLKY9g.png" alt="Image for post" style="zoom:67%;" />

- `EOTask` : task performed on `EOPatch`.  Packages of tasks are :
  - `eo-learn-core` : Basic blocs defined here (`EOPatch` , `EOTask`, `EOWorkflow`) and commonly used functionalities.
  - `eo-learn-io` : Input/output which allows to obtain data from *Sentinel Services* and *Geopedia*.
  - `eo-learn-mask` : tasks for masking clouds
  - `eo-learn-features` : tasks for extracting data properties and features manipulation (*like temporal and [Haralick features](#haralick), as well as interpolation tasks*)
  - `eo-learn-geometry` : geometric transformation (*vector to raster transformation or :fire: sampling of label masks for generating training sets for ML methods*)
  - `eo-learn-ml-tools` : ML tasks to setup or validate a model.
  - `eo-learn-coregistration` : tasks of [coregistration techniques](#coregistration).
- `EOWorkflow` : pipeline which connects tasks in an acyclic graph : vertices are `EOTasks` and are connected by edges and flows of `EOPatch`.
- 

# Land Cover Classification with eo-learn: Part 1 [Tuto 1](https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195)

**This part focus on selection and split of  an AOI and obtainment of data though here :  Sentinel-2 band data and could masks and a vector to raster map example**

[Fichier colab](https://colab.research.google.com/drive/1h4rDL_c57q_61-mN-KOsvRu4Oi_Qyl5M#scrollTo=-5fZQE0eiPX1)

## References

### <a id="haralick">Haralick features extraction</a> 

The basis for these features is the gray-level co-occurrence matrix ( **G** in Equation [2.6](http://murphylab.web.cmu.edu/publications/boland/boland_node26.html#eqn:cho_g)). This matrix is square with dimension *N**g*, where *N**g* is the number of gray levels in the image. Element [*i*,*j*] of the matrix is generated by counting the number of times a pixel with value *i* is adjacent to a pixel with value *j* and then dividing the entire matrix by the total number of such comparisons made. Each entry is therefore considered to be the probability that a pixel with value *i* will be found adjacent to a pixel of value *j*.



 ![\begin{displaymath}\mathbf{G}=\left[ \begin{array}{cccc} p(1,1) & p(1,2) & \cdot... ...N_g,1) & p(N_g,2) & \cdots & p(N_g,N_g) \\ \end{array}\right] \end{displaymath}](http://murphylab.web.cmu.edu/publications/boland/boland_img44.gif)

### <a id="coregistration">Image co-registration</a>

Image co-registration is performed when the intention is to study two or more images in a series, typically to understand change. Images may come from the same or from different sensors, and have the same or different spatial resolutions. **The rationale of co-registration is to ensure that the images become spatially aligned** so that any feature in one image overlaps as well as possible its footprint in any other image in the series. Misalignment between images may result in the extraction of incorrect temporal profiles (or crop signatures) and may subsequently give incorrect crop mapping results.

Co-registration is normally carried out by selecting one image as the reference (base/master) to which all the other images are aligned. **The selection of the master image can be carried out by use of the image metadata**, for instance taking into consideration off-nadir angle and cloud cover percentage, with low values for these two parameters being beneficial. **The co-registration process requires identification of common features** (e.g. road junctions, stream crossings, etc.) in the master and “warping” of the other images (i.e. those to be co-registered). **The locations of  the common features are called tie points**, and these are used by the co-registration application. In most software applications, tie points are automatically generated on the basis of a few manually selected tie points. Once enough tie points have been generated, a polynomial function is used to align the warped image to the master.

### NDVI

Indice used to characterize the live green vegetation.

The pigment in plant leaves, chlorophyll, strongly absorbs visible light (from 0.4 to 0.7 µm) for use in photosynthesis. The cell structure of the leaves, on the other hand, strongly reflects near-infrared light (from 0.7 to 1.1 µm). The more leaves a plant has, the more these wavelengths of light are affected, respectively

[Wikipedia](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index)

